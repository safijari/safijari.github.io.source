#+HUGO_BASE_DIR: ../
#+HUGO_SECTION: ./blog/posts
#+OPTIONS: author:nil
* Blag
** TODOs
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: ./
  :EXPORT_FILE_NAME: ../todos
  :END:
  - [ ] Write =About= section
  - [ ] Write my resume
  - [ ] Make a tutorial video for using Numba [[https://numba.pydata.org/numba-doc/latest/user/jitclass.html][jitclasses]]
  - [ ] Make a tutorial video for using Cuda with Numba
  - [ ] Make a tutorial video for Pydantic
  - [ ] Make a video about Pyodide? https://alpha.iodide.io/notebooks/300/
  - [ ] Split blog and videos using https://github.com/panr/hugo-theme-terminal/issues/97
  - [-] Fill out entries for old notable videos:
    - [-] Spacemacs videos
      - [X] Org mode
      - [ ] Basics
      - [ ] Python
      - [ ] Magit
      - [ ] Superpowers
    - [ ] Python Videos
      - [ ] Numba
      - [ ] Threading
      - [ ] Libraries
      - [ ] Mistakes
    - [ ] RL Videos
      - [ ] Live series
      - [ ] Snake
    
** About Me
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: ./
  :EXPORT_FILE_NAME: ../about
  :END:
  I'm currently an engineer a company called Simbe Robotics 
  where I've played a leading role in computer vision and machine learning initiatives.
  Specifically I've spent a lot of time building object detection and OCR systems on home grown and curated datasets.
  I've also played an instrumental role in putting the computer vision models and pipeline in production
  and scaling it to match the needs of our growing customer base.

** TODO Write CUDA Kernels in Python using Numba          :cuda:python:numba:
   CLOSED: [2020-05-20]
  :PROPERTIES:
  :EXPORT_FILE_NAME: 2020-05-20-cuda-python-numba
  :END:
  Hai
  
** TODO A Response to "Is Numpy Faster than Python?" :python:numba:numpy:julia:
  :PROPERTIES:
  :EXPORT_FILE_NAME: 2020-05-numpy-python-julia-response
  :END:
  So I woke up this morning to my google feed recommending the article [[https://towardsdatascience.com/is-numpy-faster-than-python-e8a7363d8276][Is NumPy Faster than Python?]]
  and the first thought through my head was "uhhh ... why is that even a question?". I later realized that
  this was a follow up to the author's previous article called [[https://towardsdatascience.com/should-you-jump-pythons-ship-and-move-to-julia-ccd32e7d25d9]["Should You Jump Python's Ship and Move to Julia?"]].
  
  Upon more investigation it was clear that the author is a =Julia= evangelist with seemingly very little experience
  in =Python=. Now this is ... fine. I have a great deal of respect for =Julia= and the community around it and the 
  language has come a long way from when I first used it in grad school (as I noticed during my lengthy exploration
  of the language in a past video).
  
  {{< youtube TNoShNPoEak>}}
  
  In face in the above video I did a comparison of how well a naive =Julia= implementation stacks up against a 
  naive Numba implementation of the same simulation (they were about the same). I was then [[https://discourse.julialang.org/t/how-to-optimize-the-following-code/33209][schooled by]] some 
  folks much more knowledgable about =Julia= than I was after my 4 hours session with it and they demonstrated
  that with a bit of care =Julia= could become sufficiently faster than Numba. The fact that such a simple looking
  language could be made so performant says a lot about its potential and I'm eagerly looking forward to what
  becomes of it.
  
  But this post is not about =Julia=.
  
  This post is more of a response to the "Is Numpy Faster than Python" question, but perhaps more importantly
  it serves as an assertion that *"Even if you wanted to, you likely cannot jump =Python='s ship just yet, so 
  how about we talk about how to make the most of this unfortunately less than performant language?"*
  
  OK that's a bit of a mouthful, sorry about that. Let's get to it.
  
*** The base code
    
    What follows is code taken verbatim from the aforementioned article (formatting my own, 
    I just ran [[https://pypi.org/project/black/][black]] on it). First, the pure =Python= implementation which was compared with =Julia=:
    
    #+begin_src python
def dot(x, y):
    lst = []
    for i, w in zip(x, y):
        lst.append(i * w)
    return lst


def sq(x):
    x = [c ** 2 for c in x]
    return x


class LinearRegression:
    def __init__(self, x, y):
        # a = ((∑y)(∑x^2)-(∑x)(∑xy)) / (n(∑x^2) - (∑x)^2)
        # b = (x(∑xy) - (∑x)(∑y)) / n(∑x^2) - (∑x)^2
        if len(x) != len(y):
            pass
        # Get our Summations:
        Σx = sum(x)
        Σy = sum(y)
        # dot x and y
        xy = dot(x, y)
        # ∑dot x and y
        Σxy = sum(xy)
        # dotsquare x
        x2 = sq(x)
        # ∑ dotsquare x
        Σx2 = sum(x2)
        # n = sample size
        n = len(x)
        # Calculate a
        self.a = (((Σy) * (Σx2)) - ((Σx * (Σxy)))) / ((n * (Σx2)) - (Σx ** 2))
        # Calculate b
        self.b = ((n * (Σxy)) - (Σx * Σy)) / ((n * (Σx2)) - (Σx ** 2))

    def predict(self, xt):
        xt = [self.a + (self.b * i) for i in xt]
        return xt
    #+end_src


    The code is meant to do =Linear Regression= and the API is kind of similar to what you might expect
    from =scikit-learn= (except the initializer takes the place of the =fit= function). If I came across this code
    in a review I would make the following remarks:
    
    - The "if not this then pass" in the initializer is problematic. Kindly replace with an assert or, if you'd prefer not to raise an exception, then let's talk more about your usecase and how you might redesign this.
    - Use =Numpy= arrays and ops rather than =Python= lists and ops.
    - There is already an implementation of this in =scikit-learn=, is there any reason why you can't use that (e.g. don't want to pull in a dependency)?
    
    Now the author /did/ rewrite the code with =Numpy= (again, formatting my own and I have removed the comments):
    #+begin_src python
import numpy as np


class npLinearRegression:
    def __init__(self, x, y):
        if len(x) != len(y):
            pass
        Σx = sum(x)
        Σy = sum(y)
        xy = np.multiply(x, y)
        Σxy = sum(xy)
        x2 = np.square(x)
        Σx2 = sum(x2)
        n = len(x)
        self.a = (((Σy) * (Σx2)) - ((Σx * (Σxy)))) / ((n * (Σx2)) - (Σx ** 2))
        self.b = ((n * (Σxy)) - (Σx * Σy)) / ((n * (Σx2)) - (Σx ** 2))

    def predict(self, xt):
        xt = [self.a + (self.b * i) for i in xt]
        return xt
    #+end_src
    
    but it's a bit of an unfortunate implementation (and only gives about a 50% speedup over pure =Python= and is 5x or so slower than =Julia=). Let's 
    pretend this is the update to the previous code that was submitted after my pretend review of the above code.
    My next review would likely be as follows:
    - You appear to ultimately still be operating over ==Python== lists and there's probably a lot of casting back and forth between =ndarray= and =List=.
    - =np.multiply= and =np.square= aren't really needed (if =x= and =y= are passed as =numpy= arrays, that is).
    - The =predict= function is completely unchanged.
    
    At this point I might offer the reviewee to do some pair programming with me so I can demonstrate some of the updates
    this code can dearly benefit from. At the end of that review the code would likely look something like this:
    #+begin_src python
      import numpy as np
      class NpLinearRegression:
          def __init__(self, x: np.ndarray, y: np.ndarray):
              assert len(x) == len(y), "x and y arrays must have the same number of elements along the first axis"
              Σx = x.sum()
              Σy = y.sum()
              Σxy = (sx*sy).sum()
              Σx2 = (x**2).sum()
              n = len(x)

              self.a = ((Σy) * (Σx2)) - ((Σx * (Σxy))) / ((n * (Σx2)) - (Σx ** 2))
              self.b = ((n * (Σxy)) - (Σx * Σy)) / ((n * (Σx2)) - (Σx ** 2))
        
          def predict(self, xt: np.ndarray) -> np.ndarray:
              return self.a + (self.b * xt)
    #+end_src

* Videos
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: ./blog/videos
  :END:
** DONE Org-Mode in Spacemacs                         :youtube:org:spacemacs:
   CLOSED: [2019-05-03]
  :PROPERTIES:
  :EXPORT_FILE_NAME: org-spacemacs
  :END:
  
  {{< youtube S4f-GUxu3CY>}}
  
  #+hugo: more
  
  My second ever (and apparently most popular) video was about how to use Org-Mode in
  Spacemacs. There's many really good tutorials for Org-Mode but none really focus
  on Spacemacs specifically. I cover:
  - Basic markup syntax
  - Various shortcuts
  - Task tracking, agenda
  - Various shortcuts
    
  Check it out :smile:

* COMMENT Local Variables                                           :ARCHIVE:
 # Local Variables:
 # org-hugo-auto-export-on-save: t
 # End:
